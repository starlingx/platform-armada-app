From f380234343cbb6cb65ad54574099ca5adbca9cb4 Mon Sep 17 00:00:00 2001
From: Hediberto Cavalcante da Silva <hediberto.cavalcantedasilva@windriver.com>
Date: Thu, 4 Jan 2024 15:02:15 -0300
Subject: [PATCH] ceph-csi-cephfs: add storage-init.yaml

Signed-off-by: Hediberto Cavalcante da Silva <hediberto.cavalcantedasilva@windriver.com>
Signed-off-by: Gabriel de Ara√∫jo Cabral <gabriel.cabral@windriver.com>
---
 .../templates/storage-init.yaml               | 386 ++++++++++++++++++
 1 file changed, 386 insertions(+)
 create mode 100644 charts/ceph-csi-cephfs/templates/storage-init.yaml

diff --git a/charts/ceph-csi-cephfs/templates/storage-init.yaml b/charts/ceph-csi-cephfs/templates/storage-init.yaml
new file mode 100644
index 0000000..3ffa153
--- /dev/null
+++ b/charts/ceph-csi-cephfs/templates/storage-init.yaml
@@ -0,0 +1,386 @@
+{{/*
+#
+# Copyright (c) 2020-2024 Wind River Systems, Inc.
+#
+# SPDX-License-Identifier: Apache-2.0
+#
+*/}}
+
+kind: ClusterRole
+apiVersion: rbac.authorization.k8s.io/v1
+metadata:
+  name: cephfs-rbac-secrets-namespaces
+  labels:
+    app: {{ include "ceph-csi-cephfs.name" . }}
+    chart: {{ include "ceph-csi-cephfs.chart" . }}
+    component: {{ .Values.provisioner.name }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+  annotations:
+    "meta.helm.sh/release-name": {{ .Release.Name }}
+    "meta.helm.sh/release-namespace": {{ .Release.Namespace }}
+    "helm.sh/hook": "pre-upgrade, pre-install"
+    "helm.sh/hook-delete-policy": "before-hook-creation"
+rules:
+  - apiGroups: [""]
+    resources: ["secrets"]
+    verbs: ["get", "list", "watch", "create", "delete"]
+  - apiGroups: [""]
+    resources: ["namespaces"]
+    verbs: ["get", "create", "list", "update"]
+  - apiGroups: ["storage.k8s.io"]
+    resources: ["csidrivers"]
+    verbs: ["get", "delete"]
+
+---
+
+kind: ClusterRoleBinding
+apiVersion: rbac.authorization.k8s.io/v1
+metadata:
+  name: cephfs-rbac-secrets-namespaces
+  labels:
+    app: {{ include "ceph-csi-cephfs.name" . }}
+    chart: {{ include "ceph-csi-cephfs.chart" . }}
+    component: {{ .Values.provisioner.name }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+  annotations:
+    "meta.helm.sh/release-name": {{ .Release.Name }}
+    "meta.helm.sh/release-namespace": {{ .Release.Namespace }}
+    "helm.sh/hook": "pre-upgrade, pre-install"
+    "helm.sh/hook-delete-policy": "before-hook-creation"
+subjects:
+  - kind: ServiceAccount
+    name: {{ include "ceph-csi-cephfs.serviceAccountName.provisioner" . }}
+    namespace: {{ .Values.classdefaults.cephFSNamespace }}
+roleRef:
+  kind: ClusterRole
+  name: cephfs-rbac-secrets-namespaces
+  apiGroup: rbac.authorization.k8s.io
+
+---
+
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: cephfs-storage-init
+  namespace: {{ .Values.classdefaults.cephFSNamespace }}
+  labels:
+    app: {{ include "ceph-csi-cephfs.name" . }}
+    chart: {{ include "ceph-csi-cephfs.chart" . }}
+    component: {{ .Values.provisioner.name }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+  annotations:
+    "meta.helm.sh/release-name": {{ .Release.Name }}
+    "meta.helm.sh/release-namespace": {{ .Release.Namespace }}
+    "helm.sh/hook": "pre-upgrade, pre-install"
+    "helm.sh/hook-delete-policy": "before-hook-creation"
+data:
+  ceph.conf: |
+    #
+    # Copyright (c) 2020-2022 Wind River Systems, Inc.
+    #
+    # SPDX-License-Identifier: Apache-2.0
+    #
+
+    [global]
+      # For version 0.55 and beyond, you must explicitly enable
+      # or disable authentication with "auth" entries in [global].
+      auth_cluster_required = none
+      auth_service_required = none
+      auth_client_required = none
+
+    {{ $monitors := .Values.classdefaults.monitors }}
+    {{ range $index, $monitor := $monitors}}
+    [mon.{{- $index }}]
+    mon_addr = {{ $monitor }}
+    {{- end }}
+
+  storage-init.sh: |
+    #! /bin/bash
+
+    #
+    # Copyright (c) 2020-2024 Wind River Systems, Inc.
+    #
+    # SPDX-License-Identifier: Apache-2.0
+    #
+
+    set_pool_config(){
+      local POOL=$1
+      echo "- Checking pool..."
+      ceph osd pool stats "${POOL}" &>/dev/null
+      if [ $? -ne 0 ]; then
+        echo "- Creating pool ${POOL}:"
+        ceph osd pool create "${POOL}" "${POOL_CHUNK_SIZE}"
+        RETURN_CODE=$?
+        if [ ${RETURN_CODE} -ne 0 ]; then
+          echo "Error creating pool ${POOL} (return code: ${RETURN_CODE})"
+          exit ${RETURN_CODE}
+        fi
+      else
+        echo "Pool ${POOL} already exists"
+      fi
+
+      echo "- Enabling pool ${POOL}:"
+      ceph osd pool application enable "${POOL}" cephfs
+      RETURN_CODE=$?
+      if [ ${RETURN_CODE} -ne 0 ]; then
+        echo "Error enabling pool ${POOL} (return code: ${RETURN_CODE})"
+        exit ${RETURN_CODE}
+      fi
+
+      echo "- Setting the number of replicas:"
+      ceph osd pool set "${POOL}" size "${POOL_REPLICATION}"
+      RETURN_CODE=$?
+      if [ ${RETURN_CODE} -ne 0 ]; then
+        echo "Error setting the number of pool replicas (return code: ${RETURN_CODE})"
+        exit ${RETURN_CODE}
+      fi
+
+      echo "- Assigning crush rule:"
+      ceph osd pool set "${POOL}" crush_rule "${POOL_CRUSH_RULE_NAME}"
+      RETURN_CODE=$?
+      if [ ${RETURN_CODE} -ne 0 ]; then
+        echo "Error assigning crush rule (return code: ${RETURN_CODE})"
+        exit ${RETURN_CODE}
+      fi
+    }
+
+    # Delete old driver if current fsGroupPolicy is different from "File"
+    # See: https://github.com/ceph/ceph-csi/issues/3397
+    CURRENT_FS_GROUP_POLICY=$(kubectl describe csidriver "${CSI_DRIVER_NAME}" 2>/dev/null | grep -oP 'Fs Group Policy:\K.*' | tr -d ' ')
+    if [ -n "${CURRENT_FS_GROUP_POLICY}" ] && [ "${CURRENT_FS_GROUP_POLICY}" != "File" ]; then
+       echo "Deleting old CSI driver"
+       kubectl delete csidriver "${CSI_DRIVER_NAME}"
+       if [ $? -ne 0 ]; then
+         echo "Error deleting csi driver ${CSI_DRIVER_NAME}"
+       fi
+    fi
+
+    # Copy from read only mount to Ceph config folder
+    cp /tmp/ceph.conf /etc/ceph/
+
+    touch /etc/ceph/ceph.client.admin.keyring
+
+    # Check if ceph is accessible
+    echo "================================================="
+    echo "ceph -s"
+    echo "================================================="
+    ceph -s
+    RETURN_CODE=$?
+    if [ ${RETURN_CODE} -ne 0 ]; then
+      echo -e "Error: Ceph cluster is not accessible, check Pod logs for details. (return code: ${RETURN_CODE})"
+      exit ${RETURN_CODE}
+    fi
+
+    echo "================================================="
+    echo "Creating keyring"
+    echo "================================================="
+    KEYRING=$(ceph auth get-or-create client."${USER_ID}" mon "allow r" osd "allow rwx pool=""${POOL_NAME}""" | sed -n 's/^[[:blank:]]*key[[:blank:]]\+=[[:blank:]]\(.*\)/\1/p')
+    # Set up pool key in Ceph format
+    CEPH_USER_KEYRING=/etc/ceph/ceph.client."${USER_ID}".keyring
+    echo "${KEYRING}" > "${CEPH_USER_KEYRING}"
+    echo "Keyring ${CEPH_USER_KEYRING} created"
+
+    echo -e "\n================================================="
+    echo "Creating user secret"
+    echo "================================================="
+    if [ -n "${CEPH_USER_SECRET}" ]; then
+      # check if the secret exists or is an old pattern, if not create a new one.
+      kubectl describe secret -n "${NAMESPACE}" "${CEPH_USER_SECRET}" 2>/dev/null | grep -qE "^userID"
+      if [ $? -ne 0 ]; then
+        kubectl get secret -n "${NAMESPACE}" "${CEPH_USER_SECRET}" &>/dev/null
+        if [ $? -eq 0 ]; then
+          echo "- Deleting old ${CEPH_USER_SECRET} secret for namespace ${NAMESPACE}:"
+          kubectl delete secret -n "${NAMESPACE}" "${CEPH_USER_SECRET}"
+          RETURN_CODE=$?
+          if [ ${RETURN_CODE} -ne 0 ]; then
+            echo "Error deleting secret ${CEPH_ADMIN_SECRET} for namespace ${NAMESPACE} (return code: ${RETURN_CODE})"
+            exit ${RETURN_CODE}
+          fi
+        fi
+        echo "- Creating ${CEPH_USER_SECRET} secret for namespace ${NAMESPACE}:"
+        kubectl create secret generic -n "${NAMESPACE}" "${CEPH_USER_SECRET}" --type="kubernetes.io/cephfs" --from-literal=userKey="${KEYRING}" --from-literal=userID="${USER_ID}" --from-literal=adminKey="${KEYRING}" --from-literal=adminID="${ADMIN_ID}"
+        RETURN_CODE=$?
+        if [ ${RETURN_CODE} -ne 0 ]; then
+          echo "Error creating secret ${CEPH_USER_SECRET} for namespace ${NAMESPACE} (return code: ${RETURN_CODE})"
+          exit ${RETURN_CODE}
+        fi
+      else
+        echo "Secret ${CEPH_USER_SECRET} for namespace ${NAMESPACE} already exists"
+      fi
+
+      echo -e "\n================================================="
+      echo "Creating secrets for additional namespaces"
+      echo "================================================="
+      # Support creating namespaces and Ceph user secrets for additional
+      # namespaces other than that which the provisioner is installed. This
+      # allows the provisioner to set up and provide PVs for multiple
+      # applications across many namespaces.
+      if [ -n "${ADDITIONAL_NAMESPACES}" ]; then
+        for ns in $(
+          IFS=,
+          echo ${ADDITIONAL_NAMESPACES}
+        ); do
+          kubectl get namespace "${ns}" &>/dev/null
+          if [ $? -ne 0 ]; then
+            kubectl create namespace "${ns}"
+            RETURN_CODE=$?
+            if [ ${RETURN_CODE} -ne 0 ]; then
+              echo "Error creating namespace ${ns} but continuing anyway (return code: ${RETURN_CODE})"
+              continue
+            fi
+          fi
+
+          # check if the secret exists or is an old pattern, if not create a new one.
+          kubectl describe secret -n "${ns}" "${CEPH_USER_SECRET}" 2>/dev/null | grep -qE "^userID"
+          if [ $? -ne 0 ]; then
+            kubectl get secret -n "${ns}" "${CEPH_USER_SECRET}" &>/dev/null
+            if [ $? -eq 0 ]; then
+              echo "- Deleting old ${CEPH_USER_SECRET} secret for namespace ${ns}:"
+              kubectl delete secret -n "${ns}" "${CEPH_USER_SECRET}"
+              RETURN_CODE=$?
+              if [ ${RETURN_CODE} -ne 0 ]; then
+                echo "Error deleting secret ${CEPH_USER_SECRET} for namespace ${ns} (return code: ${RETURN_CODE})"
+                exit ${RETURN_CODE}
+              fi
+            fi
+            echo "- Creating secret ${CEPH_USER_SECRET} for namespace ${ns}:"
+            kubectl create secret generic -n "${ns}" "${CEPH_USER_SECRET}" --type="kubernetes.io/cephfs" --from-literal=userKey="${KEYRING}" --from-literal=userID="${USER_ID}" --from-literal=adminKey="${KEYRING}" --from-literal=adminID="${ADMIN_ID}"
+            RETURN_CODE=$?
+            if [ ${RETURN_CODE} -ne 0 ]; then
+              echo "Error creating secret ${CEPH_USER_SECRET} for namespace ${ns} but continuing anyway (return code: ${RETURN_CODE})"
+            fi
+          else
+            echo "Secret ${CEPH_USER_SECRET} for namespace ${ns} already exists"
+          fi
+        done
+      fi
+    fi
+
+
+    echo -e "\n================================================="
+    echo "Setting pool configuration"
+    echo "================================================="
+
+    set_pool_config ${POOL_NAME}
+
+    echo -e "\n================================================="
+    echo "Setting metadata pool configuration"
+    echo "================================================="
+
+    set_pool_config ${METADATA_POOL_NAME}
+
+    echo "- Checking filesystem..."
+    ceph fs ls | grep "${FS_NAME}" &>/dev/null
+    if [ $? -ne 0 ]; then
+      echo "- Creating filesystem ${FS_NAME}:"
+      ceph fs new "${FS_NAME}" "${METADATA_POOL_NAME}" "${POOL_NAME}"
+      RETURN_CODE=$?
+      if [ ${RETURN_CODE} -ne 0 ]; then
+        echo "Error creating filesystem ${FS_NAME} (return code: ${RETURN_CODE})"
+        exit ${RETURN_CODE}
+      fi
+    else
+      echo "Filesystem ${FS_NAME} already exists"
+    fi
+
+    # subvolumeGroup is no longer created automatically in v3.10.1
+    # See: https://github.com/ceph/ceph-csi/issues/4185
+    echo -e "\n================================================="
+    echo "Creating subvolumeGroup csi"
+    echo "================================================="
+
+    # There is no need to check if the subvolumegroup already exists.
+    # The following create command does not duplicate the subvolumegroup.
+    ceph fs subvolumegroup create ${FS_NAME} csi
+    RETURN_CODE=$?
+    if [ ${RETURN_CODE} -ne 0 ]; then
+      echo -e "Error creating subvolumegroup csi (return code: ${RETURN_CODE})"
+      exit ${RETURN_CODE}
+    else
+      echo "subvolumeGroup csi for ${FS_NAME} created"
+    fi
+
+    echo -e "\n================================================="
+    echo "ceph -s"
+    echo "================================================="
+    ceph -s
+    RETURN_CODE=$?
+    if [ ${RETURN_CODE} -ne 0 ]; then
+      echo -e "Error: Ceph cluster is not accessible, check Pod logs for details. (return code: ${RETURN_CODE})"
+      exit ${RETURN_CODE}
+    fi
+
+---
+
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: cephfs-storage-init
+  namespace: {{ .Values.classdefaults.cephFSNamespace }}
+  labels:
+    app: {{ include "ceph-csi-cephfs.name" . }}
+    chart: {{ include "ceph-csi-cephfs.chart" . }}
+    component: {{ .Values.provisioner.name }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+  annotations:
+    "meta.helm.sh/release-name": {{ .Release.Name }}
+    "meta.helm.sh/release-namespace": {{ .Release.Namespace }}
+    "helm.sh/hook": "post-install, pre-upgrade, pre-rollback"
+    "helm.sh/hook-delete-policy": "before-hook-creation"
+spec:
+  backoffLimit: 5
+  template:
+    spec:
+      serviceAccountName: {{ include "ceph-csi-cephfs.serviceAccountName.provisioner" . }}
+      volumes:
+      - name: cephfs-storage-init-configmap-volume
+        configMap:
+          name: cephfs-storage-init
+          defaultMode: 0555
+      containers:
+        {{- range $sc := .Values.storageClasses }}
+        - name: storage-init-{{- $sc.name }}
+          image: "{{ $.Values.storage_init.image.repository }}:{{ $.Values.storage_init.image.tag }}"
+          command: ["/bin/bash", "/tmp/storage-init.sh"]
+          env:
+            - name: NAMESPACE
+              value: {{ $.Values.classdefaults.cephFSNamespace }}
+            - name: ADDITIONAL_NAMESPACES
+              value: {{ join "," $sc.additionalNamespaces | quote }}
+            - name: CEPH_USER_SECRET
+              value: {{ $sc.userSecretName }}
+            - name: USER_ID
+              value: {{ $sc.userId }}
+            - name: ADMIN_ID
+              value: {{ $.Values.classdefaults.adminId }}
+            - name: POOL_NAME
+              value: {{ $sc.data_pool_name }}
+            - name: METADATA_POOL_NAME
+              value: {{ $sc.metadata_pool_name }}
+            - name: FS_NAME
+              value: {{ $sc.fs_name }}
+            - name: POOL_CHUNK_SIZE
+              value: {{ $sc.chunk_size | quote }}
+            - name: POOL_REPLICATION
+              value: {{ $sc.replication | quote }}
+            - name: POOL_CRUSH_RULE_NAME
+              value: {{ $sc.crush_rule_name | quote }}
+            - name: CSI_DRIVER_NAME
+              value: {{ $.Values.driverName }}
+          volumeMounts:
+          - name: cephfs-storage-init-configmap-volume
+            mountPath: /tmp
+        {{- end }}
+      restartPolicy: OnFailure
+{{- if .Values.provisioner.nodeSelector }}
+      nodeSelector:
+{{ .Values.provisioner.nodeSelector | toYaml | trim | indent 8 }}
+{{- end }}
+{{- with .Values.provisioner.tolerations }}
+      tolerations:
+{{ toYaml . | indent 8 }}
+{{- end}}
--
2.34.1
